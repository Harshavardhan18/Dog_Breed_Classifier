{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dog_Breed_Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshavardhan18/Dog_Breed_Classifier/blob/master/Dog_Breed_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "roi2gDH-n6RJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **DOG BREED CLASSIFIER**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Bq6613i6olG4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the below cell to import all necessary library required for building the model."
      ]
    },
    {
      "metadata": {
        "id": "7M0jHcDXXsbT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_files\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing import image    \n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from keras.layers import Dropout, Flatten, Dense, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from PIL import ImageFile    \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import cv2                \n",
        "import matplotlib.pyplot as plt                        \n",
        "%matplotlib inline   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T-A1DQkRr-HR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the below cell **only if** your are using google drive to store your dataset.\n",
        "\n",
        "The below code statement is responsible for connecting to your google drive.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "9rPFxN7OK_wT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lIa7uKdRs6Km",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function to load train, test, and validation datasets and print the stats about the dataset.\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "\n",
        "1.   Change the number of class labels in load_dataset().\n",
        "\n",
        "> Example: If you want to predict for only four breed, then change number of classes to 4.\n",
        "\n",
        "\n",
        "2.   Provide path for training, validation and test dataset.(If in case you are using google drive then provide the path accordingly)\n",
        "\n",
        "\n",
        "3.   **Only edit** the content enclosed within ****\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "RPXJtIRKYRjD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define function to load train, test, and validation datasets\n",
        "def load_dataset(path):\n",
        "    data = load_files(path)\n",
        "    dog_files = np.array(data['filenames'])\n",
        "    dog_targets = np_utils.to_categorical(np.array(data['target']), #change to number of classification label )\n",
        "    return dog_files, dog_targets\n",
        "\n",
        "# load train, test, and validation datasets\n",
        "train_files, train_targets = load_dataset('**PATH TO TRAINING DATASET**')\n",
        "valid_files, valid_targets = load_dataset('**PATH TO VALIDATION DATASET**')\n",
        "test_files, test_targets = load_dataset('**PATH TO TESTING DATASET**')\n",
        "\n",
        "# load list of dog names\n",
        "dog_names = [item[20:-1] for item in sorted(glob(\"**PATH TO TRAINING DATASET**/*/\"))]\n",
        "\n",
        "# print statistics about the dataset\n",
        "print(dog_names)\n",
        "print('There are %d total dog categories.' % len(dog_names))\n",
        "print('There are %d total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
        "print('There are %d training dog images.' % len(train_files))\n",
        "print('There are %d validation dog images.' % len(valid_files))\n",
        "print('There are %d test dog images.'% len(test_files))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SAyzKJc7ve4Q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Function that converts an image to numpy array with target size of 224x224x3 and finally returns a stack of list of images that have been converted as numpy array.\n",
        "\n",
        "**Option**:\n",
        "        You many wish to change the target size of the image.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DppNWj7-bsNT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def path_to_tensor(img_path):\n",
        "    # loads RGB image as PIL.Image.Image type\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
        "    x = image.img_to_array(img)\n",
        "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
        "    return np.expand_dims(x, axis=0)\n",
        "\n",
        "def paths_to_tensor(img_paths):\n",
        "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
        "    return np.vstack(list_of_tensors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mP8FFJsA0EXt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Call's the above fuction with the train, test, and validation list and normalize the data in the returned array."
      ]
    },
    {
      "metadata": {
        "id": "Akyp9SdhioQP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# pre-process the data (Normalize)\n",
        "\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
        "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
        "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
        "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4pji5ZJE0a8M",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Perform image augmentation using **ImageDataGenerator** \n",
        "\n",
        "**Option:**\n",
        "You may choose to change the arguments such as flipping, shifs by changing range to 15% or 20% or any of your choice.\n"
      ]
    },
    {
      "metadata": {
        "id": "HbwTFFHniq-p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Image Augmentation\n",
        "# create and configure augmented image generator\n",
        "datagen = ImageDataGenerator(\n",
        "                    width_shift_range=0.1,   \n",
        "                    height_shift_range=0.1,  \n",
        "                    horizontal_flip=True) \n",
        "\n",
        "# fit augmented image generator on data\n",
        "datagen.fit(train_tensors)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hyoZdoCC-Px9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Define a model using Sequental() in keras consisting of \n",
        "\n",
        "1.   Input layer as Batch Normalization\n",
        "2.   6 layers of Convolution, Pooling and Batch normalization as regulizer and activation function as Relu (Rectified Linear Unit)\n",
        "3.   Flatten the layer.\n",
        "4.   5 hidden ouput layer each having Relu as its activation function\n",
        "5.   An output layer consisting of number of target labels and activation function as softmax which gives propability distribution value over the list of target classes.\n",
        "\n",
        "\n",
        "\n",
        "**TODO:**\n",
        "\n",
        "1.  If in case you have added more or less number of dog breed to train, then change number of nodes in the last layer to number of target classes. \n",
        "\n",
        "\n",
        "**Options:** You may choose to experiment the model by \n",
        "\n",
        "1.   Adding or removing the CNN layers.\n",
        "2.   Remove Batch Normalization.\n",
        "3.   Change the filter size, kernal size or activation function to sigmoid, tanh to see the results.\n",
        "4.   Modify pool size it is usually 1 or 2.\n",
        "5.   Add or remove dense layer.\n",
        "6.   Change the activation function in dense layer.\n",
        "7.   Change the value in dropout, usually inbetween 0.25 to 0.5 or as much as you prefer for experimenting purpose.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "tMeTiJOemIj5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Define a CNN Model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(BatchNormalization(input_shape=(224, 224, 3)))\n",
        "model.add(Conv2D(filters=16, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(filters=512, kernel_size=3, kernel_initializer='he_normal', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(32,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(16,activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RqzaYypK1eDw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Compile the model using **adam** optimizer.\n",
        "\n",
        "1.   Learning rate of 0.001.\n",
        "2.   Loss function as categorical cross entropy.\n",
        "3.   metrics which defines accuracy of the model.\n",
        "\n",
        "**Option:** Feel free to change the learning rate or loss functions any of your choice.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ats4fZA1ivrp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "\n",
        "adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mqZNYVtu2fEG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluate the model before training. The model returns two values:\n",
        "1.   Initial loss value.\n",
        "2.   Initial metrics value which gives how good the model performs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TMcnkZu6nwr0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Evaluate The model\n",
        "model.evaluate(x=valid_tensors, y=valid_targets, batch_size=None, verbose=1, sample_weight=None, steps=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0BFR-5By3QMR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Train the model by choosing the parametes as:\n",
        "\n",
        "1.   epochs = 50\n",
        "2.   batch size = 128\n",
        "\n",
        "**TODO:**\n",
        "1.   Provide the path for storing the check point **.hdf5** format (If in case you are using google drive then provide the path accordingly)\n",
        "2.   **Only edit** the content enclosed within ****\n",
        "\n",
        "**Option:** \n",
        "You may choose to experiment by changing\n",
        "1.   Number of epochs\n",
        "2.   Batch size\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DqhyIB0yi2Mu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "epochs = 50\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "checkpointer = ModelCheckpoint(filepath='**PROVIDE A PATH TO STORE THE WEIGHT FILE IN THE DRIVE IN .hdf5 FORMAT**', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "# Using Image Augmentation\n",
        "model.fit_generator(datagen.flow(train_tensors, train_targets, batch_size=batch_size),\n",
        "                    validation_data=(valid_tensors, valid_targets), \n",
        "                    steps_per_epoch=train_tensors.shape[0] // batch_size,\n",
        "                    epochs=epochs, callbacks=[checkpointer], verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvesMDtI41xJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Evaluate the model after training. The model returns two values:\n",
        "1.   Loss value.\n",
        "2.   Metrics value which gives how good the model performs."
      ]
    },
    {
      "metadata": {
        "id": "960ayrlh9ZGU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.evaluate(x=valid_tensors, y=valid_targets, batch_size=None, verbose=1, sample_weight=None, steps=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "u62aiihq94pp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gets index of predicted dog breed for each image in test set and prints the accuracy of the model on the test dataset."
      ]
    },
    {
      "metadata": {
        "id": "L6PKbAjLppDN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get index of predicted dog breed for each image in test set\n",
        "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
        "\n",
        "# report test accuracy\n",
        "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
        "print('Test accuracy: %.4f%%' % test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aW_5PQBR60xj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the cell if you want to save your model and the weight.\n",
        "\n",
        "**TODO:**\n",
        "1.   Provide the path where the model and the weight file is to be stored(If in case you are using google drive then provide the path accordingly)\n",
        "2.       **Only edit** the content enclosed within ****\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "dZU-L6Ksrd8I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.save(\"**PATH WHERE THE MODEL IS TO BE STORED IN .h5 FORMAT**\")\n",
        "model.save_weights(\"**PATH WHERE THE WEIGHT FILE IS TO BE STORED IN .hdf5 FORMAT**\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZyqgQhgu8L5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load the model and its corresponding weight file that is stored in the path.\n",
        "\n",
        "**TODO:**\n",
        "1.   Provide the path where the model and the weight file is stored(If in case you are using google drive then provide the path accordingly)\n",
        "2.       **Only edit** the content enclosed within ****\n",
        "3.       Modify the** dog_breed** variable to include the breed names for which you have trained your model to classify.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DyrQYzfz47tl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dog_breed = ['Affenpinscher','Afghan_hound','Australian_shepherd','Bichon_frise','Golden_retriever']\n",
        "\n",
        "model = load_model('**PATH WHERE THE MODEL IS STORED IN .h5 FORMAT**')\n",
        "model.load_weights('**PATH WHERE THE WEIGHT FILE IS STORED IN .hdf5 FORMAT**')\n",
        "model.summary()\n",
        "\n",
        "def predict_breed(np_arr):\n",
        "    return np.argmax(model.predict(np.expand_dims(np_arr, axis=0)))\n",
        "\n",
        "def dog_breed_classifier(np_arr, img_path, output):\n",
        "    find_breed = predict_breed(np_arr)\n",
        "    img = cv2.imread(img_path)\n",
        "    cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "    print(\"The correct dog breed is \" + dog_breed[np.argmax(output) - 1])\n",
        "    print(\"The detected dog breed is \" + dog_breed[find_breed - 1])\n",
        "    plt.imshow(cv_rgb)\n",
        "    plt.show()\n",
        "    \n",
        "for img, nparr, output in zip(test_files, test_tensors, test_targets):\n",
        "    dog_breed_classifier(nparr, img, output)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}